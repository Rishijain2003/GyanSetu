import os
import streamlit as st
import time 
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# Updated paths
DB_FAISS_PATH = "faiss_hindi_index_e5"
MODEL_NAME = "intfloat/multilingual-e5-base"

st.set_page_config(page_title="Gyan Setu v2", page_icon="üïâÔ∏è", layout="centered")

@st.cache_resource
def load_embeddings_model():
    st.write(f"üî§ Loading multilingual embedding model: {MODEL_NAME}")
    return HuggingFaceEmbeddings(
        model_name=MODEL_NAME,
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

@st.cache_resource
def load_faiss_store(embeddings):
    if not os.path.exists(DB_FAISS_PATH):
        st.error(f"FAISS database not found: {DB_FAISS_PATH}")
        st.stop()
    st.write("üìÇ Loading FAISS vector database...")
    return FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

def create_rag_chain(vectorstore):
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 20})

    template = """‡§Ü‡§™ ‡§è‡§ï ‡§Æ‡§¶‡§¶‡§ó‡§æ‡§∞ AI ‡§∏‡§π‡§æ‡§Ø‡§ï ‡§π‡•à‡§Ç ‡§ú‡•ã ‡§ï‡•á‡§µ‡§≤ ‡§®‡§ø‡§Æ‡•ç‡§®‡§≤‡§ø‡§ñ‡§ø‡§§ ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§ï‡•á ‡§Ü‡§ß‡§æ‡§∞ ‡§™‡§∞ ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§
    ‡§Ø‡§¶‡§ø ‡§â‡§§‡•ç‡§§‡§∞ ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§Æ‡•á‡§Ç ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à, ‡§§‡•ã ‡§µ‡§ø‡§®‡§Æ‡•ç‡§∞‡§§‡§æ ‡§∏‡•á ‡§ï‡§π‡•á‡§Ç ‡§ï‡§ø ‡§Ü‡§™‡§ï‡•ã ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤ ‡§™‡§æ‡§à‡•§

    Context: {context}
    Question: {question}

    Hindi Answer:"""
    prompt = ChatPromptTemplate.from_template(template)

    return (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

def main():
    st.title("üïâÔ∏è ‡§ú‡•ç‡§û‡§æ‡§® ‡§∏‡•á‡§§‡•Å v2 (Multilingual Gita RAG)")
    st.markdown("##### Powered by `intfloat/multilingual-e5-base` embeddings")

    if "OPENAI_API_KEY" not in os.environ:
        st.warning("Missing OpenAI API key. Please set it before running.")
        st.stop()

    embeddings = load_embeddings_model()
    vectorstore = load_faiss_store(embeddings)
    rag_chain = create_rag_chain(vectorstore)

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    if prompt := st.chat_input("‡§Ö‡§™‡§®‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§™‡•Ç‡§õ‡•á‡§Ç..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("Generating response..."):
                response = rag_chain.invoke(prompt)
                st.markdown(response)

        st.session_state.messages.append({"role": "assistant", "content": response})

if __name__ == "__main__":
    main()
